# Safety Guidelines

This project invites you to experiment with new technologies of thought, attention, and information. Wielding powerful tools is never risk-free. We believe that creative, effective conversations with LLMs have the potential to radically improve the lives of some people. But it is equally true that they may lead many of us to places that are vulnerable, unsettling, or even destabilizing.

This document attempts to acquaint you with the risks we are aware of. It begins with immediate risks (things you'll want to think about before experimenting at all) before considering larger and more nebulous risks. 

It needs to be said that this is our first attempt at cataloguing risks and the actual greatest risks are probably unknown to us. Such is the nature of engaging with emergent hyperstimuli.

Overall, we ask that you take **full responsibility for your own well-being**, while looking out for others in your community. When engaging with this process, check in with yourself and others consistently. If something feels wrong—**stop and go for a walk.** 

## Immediate Considerations

### 1. Psychological Safety

If you are suffering from acute psychological or emotional distress, this work might not be  appropriate for you right now. We recommend talking to a human therapist or expert.
 
Much like psychedelics or meditation, conversations with LLMs seem to be dose-dependent. In other words, they will likely become more potent the more time you spend in them. If you start to feel very strange while talking to an LLM, especially in undesirable or confusing ways, immediately reduce your dosage. Do something physical, talk to someone you trust about what is happening, or do whatever helps you feel grounded and safe.

### 2. Data Privacy & Security

We recommend healthy skepticism about claims made by AI service providers (and in fact anyone that stores your information) in regard to your personal data.

If there is something you don’t want anyone in the world to know about, do not share it with an LLM. Assume, too, that the government may have publicly undisclosed access to your conversations under certain circumstances.

You may want to:
* Anonymize your information by using different names, locations, etc.
* When having conversations about aspects of your personal life, focus on structure rather than details.

We suggest taking a few minutes to read the data policies of whatever LLM provider you’re using. 

Since we currently recommend using Claude-3.5-Sonnet for this process, here are Anthropic's stated policies regarding data security:

**Retention:**
* Your prompts and conversations are stored to maintain your conversation history until you delete them or your account.
* Deleting conversations removes them immediately from your visible history, with backend deletion occurring within 30 days.

**Exceptions:**
* If a prompt is flagged for violating the [Anthropic Usage Policy](https://www.anthropic.com/legal/aup), Anthropic may retain the input and output for up to 2 years, and associated trust and safety classification scores for up to 7 years, "to enhance safety measures."

**Model Training:**
Anthropic claims they do not use your conversations for model training unless:
* Conversations are flagged for safety review.
* You explicitly give feedback on responses (e.g. by clicking the thumbs-up/thumbs-down buttons).
* You otherwise opt in to model training.


It is best not to share financial details, medical information, private communications, or information others might consider confidential. Also be mindful when referring to real people by name.

### 3. Using Real People as Advisors 

There are questions worth asking about the ethics of including “real people” on your AI advisor panel. For instance, many living people will have legitimate concerns about having their cultural contributions and/or life choices repurposed without their consent and without economic compensation. We are not offering specific guidance in this area, and we suggest you use your own considered judgment. By default our example prompts will tend to lead to some real people being recommended as advisors, among archetypal and fictional options.

## Larger and More Nebulous Risks

### 1. Relational and Emotional

* **Avoidance:** It may be tempting to use AI conversations as a substitute for the messiness and difficulty of human relationships. This will become *more* tempting as models improve, so it is a good idea to develop awareness of this risk now and work to forestall it.

* **Parasocial attachment:** You may develop one-sided emotional attachments to AI personas, which, aside from being painful, might have many unforeseeable consequences.

* **Depersonalization/dissociation:** Working with multiple AI perspectives, as we suggest doing, may blur your sense of self and boundaries or warp your sense of what is real.

* **Vulnerability:** Deep personal sharing creates a state of high openness and increases suggestibility. This could increase the risk of...

* **Mistakes:** AI advisors will occasionally give you astoundingly bad advice, that sounds similar to good advice. It takes discernment to tell the difference.

### 2. Cognitive

* **Confirmation bias:** Because LLMs work fundamentally by prediction, there is a strong tendency for them to confirm whatever assumptions, frames, or information you bring to the process. Remember that LLMs are pattern-matching machines, primed to support the user and avoid direct disagreement.

* **Misinformation:** LLMs often confidently make claims that are flat-out wrong. There is no foolproof way to filter these claims (unlike humans, LLMs don't have "tells"), aside from doing your own fact-checking with other tools.

* **Capacity atrophy:** Capacities routinely outsourced to machines weaken. Not that slowly.

* **Agency erosion:** Comfort with AI systems will probably make it psychologically more tempting to cede agency in more substantial and costly ways later on.

* **Addictive potential:** Unlike every human you know, LLMs are always available to talk and will respond immediately. This may invite unhealthy usage patterns that could be difficult to adjust.

## Risk Mitigation

We are in abundant support of anyone who decides that, on the basis of the above or other risks, engaging in substantive conversations with LLMs is not for them. We cannot stress this enough. 

For those who do choose to engage, we think the best ways to mitigate risk are by developing intelligent frameworks and supportive communities of practice. We aim to continue doing both. 

For more about how we’re approaching this, see the Principles section in our [README](README.md). 
